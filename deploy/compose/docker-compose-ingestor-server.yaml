services:
# Main ingestor server which is responsible for ingestion
  ingestor-server:
    container_name: ingestor-server
    image: nvcr.io/nvidia/blueprint/ingestor-server:${TAG:-2.0.0}
    build:
      # Set context to repo's root directory
      context: ../../
      dockerfile: src/ingestor_server/Dockerfile
    # start the server on port 8082 with 4 workers for improved latency on concurrent requests.
    command: --port 8080 --host 0.0.0.0 --workers 1

    # Common customizations to the pipeline can be controlled using env variables
    environment:
      # Path to example directory relative to root
      EXAMPLE_PATH: 'src/ingestor_server'

      ##===Vector DB specific configurations===
      # Pinecone configuration 
      PINECONE_API_KEY: pclocal
      PINECONE_HOST: http://pinecone-local:5080
      PINECONE_INDEX_NAME: rag-index

      # vectorstore collection name to store embeddings
      COLLECTION_NAME: ${COLLECTION_NAME:-multimodal_data}

      NVIDIA_API_KEY: ${NVIDIA_API_KEY:?"NVIDIA_API_KEY is required"}

      ##===Embedding Model specific configurations===
      # url on which embedding model is hosted. If "", Nvidia hosted API is used
      APP_EMBEDDINGS_SERVERURL: ${APP_EMBEDDINGS_SERVERURL-"nemoretriever-embedding-ms:8000"}
      APP_EMBEDDINGS_MODELNAME: ${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}
      APP_EMBEDDINGS_DIMENSIONS: ${APP_EMBEDDINGS_DIMENSIONS:-2048}

      ##===NV-Ingest Connection Configurations=======
      APP_NVINGEST_MESSAGECLIENTHOSTNAME: ${APP_NVINGEST_MESSAGECLIENTHOSTNAME:-"nv-ingest-ms-runtime"}
      APP_NVINGEST_MESSAGECLIENTPORT: ${APP_NVINGEST_MESSAGECLIENTPORT:-7670}

      ##===NV-Ingest Extract Configurations==========
      APP_NVINGEST_EXTRACTTEXT: ${APP_NVINGEST_EXTRACTTEXT:-True}
      APP_NVINGEST_EXTRACTTABLES: ${APP_NVINGEST_EXTRACTTABLES:-True}
      APP_NVINGEST_EXTRACTCHARTS: ${APP_NVINGEST_EXTRACTCHARTS:-True}
      APP_NVINGEST_EXTRACTIMAGES: ${APP_NVINGEST_EXTRACTIMAGES:-False}
      APP_NVINGEST_EXTRACTMETHOD: ${APP_NVINGEST_EXTRACTMETHOD:-pdfium}
      # Extract text by "page" only recommended for documents with pages like .pdf, .docx, etc.
      APP_NVINGEST_TEXTDEPTH: ${APP_NVINGEST_TEXTDEPTH:-page} # extract by "page" or "document"

      ##===NV-Ingest Splitting Configurations========
      APP_NVINGEST_CHUNKSIZE: ${APP_NVINGEST_CHUNKSIZE:-1024}
      APP_NVINGEST_CHUNKOVERLAP: ${APP_NVINGEST_CHUNKOVERLAP:-150}

      ##===NV-Ingest Caption Model configurations====
      APP_NVINGEST_CAPTIONMODELNAME: ${APP_NVINGEST_CAPTIONMODELNAME:-"meta/llama-3.2-11b-vision-instruct"}
      # Incase of nvidia-hosted caption model, use the endpoint url as - https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions
      APP_NVINGEST_CAPTIONENDPOINTURL: ${APP_NVINGEST_CAPTIONENDPOINTURL:-"http://vlm-ms:8000/v1/chat/completions"}

      # Choose whether to store the extracted content in the vector store for citation support
      ENABLE_CITATIONS: ${ENABLE_CITATIONS:-True}

      # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
      LOGLEVEL: ${LOGLEVEL:-INFO}

      # Minio configuration for binary content storage
      MINIO_ENDPOINT: minio:9010
      MINIO_ACCESSKEY: minioadmin
      MINIO_SECRETKEY: minioadmin
      MINIO_BUCKET: citations

    ports:
      - "8082:8082"
    expose:
      - "8082"
    shm_size: 5gb
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    #depends_on:
    #  pinecone-local:
    #    condition: service_healthy
    #  minio:
    #    condition: service_healthy
    networks:
      - nvidia-rag

  redis:
    image: "redis/redis-stack"
    ports:
      - "6379:6379"

  nv-ingest-ms-runtime:
    image: nvcr.io/nvidia/nemo-microservices/nv-ingest:25.3.0
    platform: linux/amd64
    volumes:
      - ${DATASET_ROOT:-./data}:/workspace/data
    ports:
      - "7670:7670"
      - "7671:7671"
    environment:
      # Simplify configuration - disable optional features
      - APP_NVINGEST_EXTRACTTABLES=False
      - APP_NVINGEST_EXTRACTCHARTS=False
      - APP_NVINGEST_EXTRACTIMAGES=False
      - MAX_INGEST_PROCESS_WORKERS=1
      - READY_CHECK_ALL_COMPONENTS=False
      - PYTHONUNBUFFERED=1
      - LOGLEVEL=DEBUG
      # Basic required configs
      - MESSAGE_CLIENT_HOST=redis
      - MESSAGE_CLIENT_PORT=6379
      - MESSAGE_CLIENT_TYPE=redis
      - NVIDIA_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      # Disable unused services
      - PADDLE_INFER_PROTOCOL=none
      - YOLOX_INFER_PROTOCOL=none
      - YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL=none
      - YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL=none
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    networks:
      - nvidia-rag
    restart: on-failure:3

networks:
  default:
    name: nvidia-rag